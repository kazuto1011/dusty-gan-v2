<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Generative Range Imaging for Learning Scene Priors of 3D LiDAR Data (WACV 2023)</title>

    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-Zenh87qX5JnK2Jl0vWa8Ck2rdkQ2Bzep5IDxbcnCeuOxjzrPF/et3URy9Bv1WTRi"
      crossorigin="anonymous" />
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css"
      integrity="sha512-xh6O/CkQoPOWDdYTDqeRdPCVd1SpvCA9XXcUnZS2FmJNp1coAFzvtCN9BmamE+4aHK8yyUHUSCcJHgXloTyT2A=="
      crossorigin="anonymous"
      referrerpolicy="no-referrer" />
    <script src="https://code.jquery.com/jquery-3.4.1.min.js"></script>

    <link rel="stylesheet" href="touch-image-comparison-slider/cndk.beforeafter.css" />
    <script src="touch-image-comparison-slider/cndk.beforeafter.js"></script>

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="stylesheet" type="text/css" href="css/main.css" />
    <link rel="stylesheet" type="text/css" href="css/cndk.beforeafter.css" />

    <script>
      $(document).ready(function () {
        $(".beforeafterdefault").cndkbeforeafter();
        $(".beforeafterautoslide").cndkbeforeafter({
          autoSliding: true,
          hoverEffect: false,
          theme: "dark",
          seperatorWidth: "0px",
        });
      });
    </script>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-79606002-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());
      gtag("config", "G-HJM3XH5K6G");
    </script>
  </head>

  <body>
    <div class="container header">
      <h5>
        <a href="../dusty-gan" target="_blank" rel="noopener">DUSty (2021)</a>
        / DUSty v2 (2023)
      </h5>
    </div>

    <div class="container header">
      <h1 class="title">Generative Range Imaging for Learning Scene Priors of 3D LiDAR Data</h1>
      <h5 class="authors">
        <a href="https://kazuto1011.github.io/" target="_blank" rel="noopener"> Kazuto Nakashima</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;
        <a href="http://robotics.ait.kyushu-u.ac.jp/~yumi" target="_blank" rel="noopener"> Yumi Iwashita</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;
        <a href="https://robotics.ait.kyushu-u.ac.jp/kurazume/en/" target="_blank" rel="noopener"> Ryo Kurazume</a><sup>1</sup>
      </h5>
      <h5 class="affiliations"><sup>1</sup>Kyushu University &nbsp;&nbsp;&nbsp; <sup>2</sup>Jet Propulsion Labratory, Caltech</h5>
      <h5 class="conference">WACV 2023</h5>
      <div class="materials">
        <a href="http://arxiv.org/abs/2210.11750" target="_blank" rel="noopener"><i class="fa-solid fa-file-pdf"></i></a>
        &nbsp;&nbsp;&nbsp;
        <a href="https://github.com/kazuto1011/dusty-gan-v2" target="_blank" rel="noopener"><i class="fa-brands fa-github"></i></a>
      </div>
    </div>

    <div class="container content">
      <figure>
        <img src="img/kitti.gif" style="width: 512; aspect-ratio: 512/64; object-fit: cover; object-position: center 100%" />
        <figcaption>Training data from KITTI</figcaption>
      </figure>
      <div style="margin: 0 0 0.2rem 0; text-align: center">
        <div class="arrow-bottom"></div>
      </div>
      <figure>
        <img src="img/interpolation.gif" style="width: 100%; aspect-ratio: 512/64; object-fit: cover; object-position: center 100%" />
        <figcaption>Sampling from our learned priors</figcaption>
      </figure>
      <div class="beforeafterautoslide">
        <div data-type="data-type-image">
          <div data-type="before" data-title="Complete depth">
            <img src="img/interpolation.gif" style="width: 1000px; aspect-ratio: 512/64; object-fit: cover; object-position: center 0%" />
          </div>
          <div data-type="after" data-title="Ray-drop probability">
            <img src="img/interpolation.gif" style="width: 1000px; aspect-ratio: 512/64; object-fit: cover; object-position: center 50%" />
          </div>
        </div>
      </div>
    </div>

    <div class="container content">
      <h2>Abstract</h2>
      <b>TL;DR: We propose GAN-based resolution-free data priors for LiDAR domain adaptation</b>
      <p>
        3D LiDAR sensors are indispensable for the robust vision of autonomous mobile robots. However, deploying LiDAR-based perception algorithms
        often fails due to a domain gap from the training environment, such as inconsistent angular resolution and missing properties. Existing
        studies have tackled the issue by learning inter-domain mapping, while the transferability is constrained by the training configuration and
        the training is susceptible to peculiar lossy noises called ray-drop. To address the issue, this paper proposes a generative model of LiDAR
        range images applicable to the data-level domain transfer. Motivated by the fact that LiDAR measurement is based on point-by-point range
        imaging, we train an implicit image representation-based generative adversarial networks along with a differentiable ray-drop effect. We
        demonstrate the fidelity and diversity of our model in comparison with the point-based and image-based state-of-the-art generative models. We
        also showcase upsampling and restoration applications. Furthermore, we introduce a Sim2Real application for LiDAR semantic segmentation. We
        demonstrate that our method is effective as a realistic ray-drop simulator and outperforms state-of-the-art methods.
      </p>
    </div>

    <div class="container content">
      <h2>Reconstruction</h2>
      <p>
        Reconstruction by the optimization-based GAN inversion
        <a href="https://arxiv.org/abs/1809.08495">[Roich et al. TOG'22]</a>.
      </p>
      <div class="row">
        <div class="col">
          <figure>
            <img src="img/inversion/inversion_0000004621_ref.png" />
            <figcaption></figcaption>
          </figure>
          <div style="margin: 0 0 0.2rem 0; text-align: center">
            <div class="arrow-bottom"></div>
          </div>
          <div class="beforeafterautoslide">
            <div data-type="data-type-image">
              <div data-type="before" data-title="">
                <img src="img/inversion/inversion_0000004621_gen_d_orig.png" />
              </div>
              <div data-type="after" data-title="">
                <img src="img/inversion/inversion_0000004621_gen_noise_p.png" />
              </div>
            </div>
          </div>
          <figure>
            <img src="img/inversion/inversion_0000004621.gif" /><br />
            <figcaption></figcaption>
          </figure>
        </div>
        <div class="col">
          <figure>
            <img src="img/inversion/inversion_0000016085_ref.png" />
            <figcaption></figcaption>
          </figure>
          <div style="margin: 0 0 0.2rem 0; text-align: center">
            <div class="arrow-bottom"></div>
          </div>
          <div class="beforeafterautoslide">
            <div data-type="data-type-image">
              <div data-type="before" data-title="">
                <img src="img/inversion/inversion_0000016085_gen_d_orig.png" />
              </div>
              <div data-type="after" data-title="">
                <img src="img/inversion/inversion_0000016085_gen_noise_p.png" />
              </div>
            </div>
          </div>
          <figure>
            <img src="img/inversion/inversion_0000016085.gif" /><br />
            <figcaption></figcaption>
          </figure>
        </div>
      </div>
    </div>

    <div class="container content">
      <h2>Restoration</h2>
      <p>Corrupted data can also be restored by exploring learned scene priors.</p>
      <div class="row">
        <div class="col" style="text-align: center">Original</div>
        <div class="col" style="text-align: center">Sparse "rings"</div>
        <div class="col" style="text-align: center">Sparse points</div>
      </div>
      <div class="row">
        <div class="col">
          <img src="img/kitti_raw_0000017000.svg" />
        </div>
        <div class="col">
          <img src="img/kitti_raw_0000017000_sparse_lines.svg" />
        </div>
        <div class="col">
          <img src="img/kitti_raw_0000017000_sparse_points.svg" />
        </div>
      </div>
    </div>

    <div class="container content">
      <h2>Upsampling</h2>
      <p>
        The <b>1x</b> result was obtained by reconstruction. The <b>2x</b> and <b>4x</b> results can be obtained just by changing coordinate queries.
      </p>
      <div class="row">
        <div class="col">
          <figure>
            <img src="https://user-images.githubusercontent.com/9032347/198226854-38b0fba4-459f-4d44-ae88-e5bdccef1acc.gif" />
            <figcaption>Target from KITTI</figcaption>
          </figure>
        </div>
        <div class="col">
          <figure>
            <img src="https://user-images.githubusercontent.com/9032347/198226734-4cc7687c-8b31-4ef6-b23a-34ac3f6d5c95.gif" />
            <figcaption>1x</figcaption>
          </figure>
        </div>
        <div class="col">
          <figure>
            <img src="https://user-images.githubusercontent.com/9032347/198226790-cd595dec-9daa-4ecc-9a8d-83654fd88c2d.gif" />
            <figcaption>2x</figcaption>
          </figure>
        </div>
        <div class="col">
          <figure>
            <img src="https://user-images.githubusercontent.com/9032347/198226816-21c115cb-0aab-4491-92a9-4231f9c1ff22.gif" />
            <figcaption>4x</figcaption>
          </figure>
        </div>
      </div>
    </div>

    <div class="container content">
      <h2>Sim2Real Semantic Segmentation</h2>
      <p>
        Our model can be used as a ray-drop noise simulator!<br />
        We conducted the semantic segmentation on KITTI annotated with
        <b style="color: blue">car</b> and <b style="color: red">pedestrian</b> classes
        <a href="https://arxiv.org/abs/1809.08495">[Wu et al. ICRA'19]</a>.<br />
        <b>Baseline</b> was trained on GTA-LiDAR only (in-game simulation w/o noise). <b>Ours</b> was trained on GTA-LiDAR w/ our noises.
      </p>
      <div class="row">
        <div class="col">
          <figure>
            <img src="https://user-images.githubusercontent.com/9032347/198207761-178f63b6-1ec8-4c61-881a-cc5265c2e817.gif" />
            <figcaption>Input</figcaption>
          </figure>
        </div>
        <div class="col">
          <figure>
            <img src="https://user-images.githubusercontent.com/9032347/198207754-e7b8c971-856e-4e56-982d-520b0fb732c4.gif" />
            <figcaption>GT</figcaption>
          </figure>
        </div>
        <div class="col">
          <figure>
            <img src="https://user-images.githubusercontent.com/9032347/198207716-710d8ced-54b8-44d5-9ed5-9d9a2ca87587.gif" />
            <figcaption>Baseline</figcaption>
          </figure>
        </div>
        <div class="col">
          <figure>
            <img src="https://user-images.githubusercontent.com/9032347/198207765-fce22492-660c-446a-8e75-7f34e8cb57f0.gif" />
            <figcaption>Ours</figcaption>
          </figure>
        </div>
      </div>
    </div>

    <div class="container content">
      <h2>Citation</h2>
      <pre class="bibtex">
<code>@inproceedings{nakashima2023generative,
    author    = {Nakashima, Kazuto and Iwashita, Yumi and Kurazume, Ryo},
    title     = {Generative Range Imaging for Learning Scene Priors of 3D LiDAR Data},
    booktitle = {IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
    pages     = {}
    year      = {2023}
}</code></pre>
    </div>

    <div class="container content">
      <h2>Related Work</h2>
      <b>Learning to Drop Points for LiDAR Scan Synthesis</b><br />
      Kazuto Nakashima and Ryo Kurazume<br />
      IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021<br />
      [<a href="../dusty-gan">Project</a>] [<a href="https://arxiv.org/abs/2102.11952">PDF</a>]
    </div>

    <div class="container content">
      <h2>Acknowledgments</h2>
      <p>
        This work was partially supported by a Grant-in-Aid for JSPS Fellows Grant Number JP19J12159, JSPS KAKENHI Grant Number JP20H00230, and JST
        Moonshot R&D Grant Number JPMJMS2032.
      </p>
    </div>
  </body>
</html>
